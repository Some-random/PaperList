# PaperList

## Improve Planning abilities using RL
* [RetroFormer](https://www.semanticscholar.org/reader/81b10e64133e775dab53153cc82277d276efe1f7): freezes the base LLM and trains reinforcement learning models to refine reflections through policy gradient methods
* [ADAPTING LLM AGENTS THROUGH COMMUNICATION](https://www.semanticscholar.org/reader/0581da0255edaf47817a899116388caf7d418273) : applies PPO training directly to an open-source LLM based on feedback and agent exploration trajectories


## Analyze/Optimize reflection
* [Text2Reward](https://www.semanticscholar.org/reader/c52b30a60fda5f23cc0d2241c4e127f5191bbb2d): transforms feedback into code to minimize feedback ambiguity
* [ExpeL: LLM Agents Are Experiential Learners](https://www.semanticscholar.org/reader/5e4597eb21a393b23e473cf66cb5ae8b27cab03e): utilizes inter-task feedback from both successes and failures to enhance model learning.
* [ALIGNING LANGUAGE MODELS WITH JUDGMENTS](https://arxiv.org/pdf/2312.14591.pdf): create contrasting samples with correct/incorrect prediction and feedback to train LLM toget better alignment


## Agent framework


## Analyze COT capabilities


## Open Domain QA Datasets


## Open Domain QA methods
